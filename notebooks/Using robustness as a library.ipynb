{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/home/ubuntu/logs_russian_roulette_adversarial_training'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barebones starter example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR\n",
    "import torch as ch\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==> Preparing dataset cifar..\nFiles already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CIFAR('/tmp/')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet18', dataset=ds)\n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Logging in: /home/ubuntu/logs_russian_roulette_adversarial_training/808a675c-0185-4c9f-b685-90a21b3eecb0\n"
    }
   ],
   "source": [
    "# Create a cox store for logging\n",
    "store = store.Store(OUT_DIR, \"CIFAR10 -- Russian Roulette Adversarial Training\")\n",
    "args_dict = args.as_dict() if isinstance(args, utils.Parameters) else vars(args)\n",
    "schema = store.schema_from_dict(args_dict)\n",
    "store.add_table('metadata', schema)\n",
    "store['metadata'].append_row(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 5,\n",
    "    'log_iters'  : 10,\n",
    "    'stop_probability' : 0.1\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Train Epoch:0 | Loss 1.9917 | AdvPrec1 29.470 | AdvPrec5 81.114 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:32<00:00,  3.03it/s]\nVal Epoch:0 | Loss 1.6042 | NatPrec1 39.170 | NatPrec5 89.770 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:00<00:00, 20.51it/s]\nVal Epoch:0 | Loss 2.1360 | AdvPrec1 20.560 | AdvPrec5 79.560 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:22<00:00,  1.14s/it]\nTrain Epoch:1 | Loss 1.5309 | AdvPrec1 43.442 | AdvPrec5 90.932 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:28<00:00,  3.49it/s]\nTrain Epoch:2 | Loss 1.2701 | AdvPrec1 53.878 | AdvPrec5 94.206 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:26<00:00,  3.72it/s]\nTrain Epoch:3 | Loss 1.0978 | AdvPrec1 60.698 | AdvPrec5 95.862 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:27<00:00,  3.60it/s]\nTrain Epoch:4 | Loss 0.9638 | AdvPrec1 65.784 | AdvPrec5 96.856 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:27<00:00,  3.61it/s]\nVal Epoch:4 | Loss 0.8916 | NatPrec1 67.810 | NatPrec5 97.870 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:00<00:00, 20.84it/s]\nVal Epoch:4 | Loss 3.6348 | AdvPrec1 10.670 | AdvPrec5 81.880 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crit = ch.nn.CrossEntropyLoss()\n",
    "def custom_train_loss(logits, targ):\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    return train_crit(logits_to_multiply * logits, targ)\n",
    "\n",
    "adv_crit = ch.nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "def custom_adv_loss(model, inp, targ):\n",
    "    logits = model(inp)\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    new_logits = logits_to_multiply * logits\n",
    "    return adv_crit(new_logits, targ), new_logits\n",
    "\n",
    "train_args.custom_train_loss = custom_train_loss\n",
    "train_args.custom_adv_loss = custom_adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Train Epoch:0 | Loss 2.9150 | AdvPrec1 12.864 | AdvPrec5 55.432 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:31<00:00,  3.13it/s]\nVal Epoch:0 | Loss 2.2236 | NatPrec1 18.270 | NatPrec5 65.570 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:01<00:00, 19.45it/s]\nVal Epoch:0 | Loss 2.2883 | AdvPrec1 15.960 | AdvPrec5 62.130 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\nTrain Epoch:1 | Loss 2.1842 | AdvPrec1 18.872 | AdvPrec5 73.456 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:30<00:00,  3.18it/s]\nTrain Epoch:2 | Loss 2.1177 | AdvPrec1 21.404 | AdvPrec5 78.918 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:29<00:00,  3.32it/s]\nTrain Epoch:3 | Loss 2.0731 | AdvPrec1 25.254 | AdvPrec5 81.684 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:33<00:00,  2.94it/s]\nTrain Epoch:4 | Loss 2.0079 | AdvPrec1 29.734 | AdvPrec5 84.780 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:30<00:00,  3.17it/s]\nVal Epoch:4 | Loss 1.9488 | NatPrec1 33.730 | NatPrec5 86.850 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:00<00:00, 20.72it/s]\nVal Epoch:4 | Loss 2.2018 | AdvPrec1 22.010 | AdvPrec5 77.550 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:29<00:00,  1.45s/it]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataParallel(\n  (module): AttackerModel(\n    (normalizer): InputNormalize()\n    (model): ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (layer1): SequentialWithArgs(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential()\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential()\n        )\n      )\n      (layer2): SequentialWithArgs(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential()\n        )\n      )\n      (layer3): SequentialWithArgs(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential()\n        )\n      )\n      (layer4): SequentialWithArgs(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (shortcut): Sequential()\n        )\n      )\n      (linear): Linear(in_features=512, out_features=10, bias=True)\n    )\n    (attacker): Attacker(\n      (normalize): InputNormalize()\n      (model): ResNet(\n        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (layer1): SequentialWithArgs(\n          (0): BasicBlock(\n            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential()\n          )\n          (1): BasicBlock(\n            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential()\n          )\n        )\n        (layer2): SequentialWithArgs(\n          (0): BasicBlock(\n            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential(\n              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (1): BasicBlock(\n            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential()\n          )\n        )\n        (layer3): SequentialWithArgs(\n          (0): BasicBlock(\n            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential(\n              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (1): BasicBlock(\n            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential()\n          )\n        )\n        (layer4): SequentialWithArgs(\n          (0): BasicBlock(\n            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential(\n              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (1): BasicBlock(\n            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (shortcut): Sequential()\n          )\n        )\n        (linear): Linear(in_features=512, out_features=10, bias=True)\n      )\n    )\n  )\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom per-iteration logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SCHEMA = {'iteration': int, 'weight_norm': float }\n",
    "out_store.add_table('custom', CUSTOM_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector as flatten\n",
    "\n",
    "def log_norm(mod, it, loop_type, inp, targ):\n",
    "    if loop_type == 'train':\n",
    "        curr_params = flatten(mod.parameters())\n",
    "        log_info_custom = { 'iteration': it,\n",
    "                    'weight_norm': ch.norm(curr_params).detach().cpu().numpy() }\n",
    "        out_store['custom'].append_row(log_info_custom)\n",
    "    \n",
    "train_args.iteration_hook = log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Must implement the num_classes argument\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        out = x.view(x.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "new_model = MLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, _ = make_and_restore_model(arch=new_model, dataset=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, new_model, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('rrat': conda)",
   "language": "python",
   "name": "python_defaultSpec_1601336393535"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}