{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barebones starter example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/utrerf/anaconda3/lib/python3.7/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR\n",
    "import torch as ch\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CIFAR('/tmp/')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet18', dataset=ds)\n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/e2d03a23-6778-4c3a-b200-105180b9ec99\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 5,\n",
    "    'log_iters'  : 10,\n",
    "    'stop_probability' : 0.1\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.1885 | AdvPrec1 24.386 | AdvPrec5 77.284 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:45<00:00,  2.15it/s]\n",
      "Val Epoch:0 | Loss 1.6648 | NatPrec1 37.850 | NatPrec5 88.830 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:01<00:00, 10.85it/s]\n",
      "Val Epoch:0 | Loss 2.1446 | AdvPrec1 19.450 | AdvPrec5 78.660 | Reg term: 0.0 ||: 100%|██████████| 20/20 [00:49<00:00,  2.49s/it]\n",
      "Train Epoch:1 | Loss 1.6106 | AdvPrec1 39.524 | AdvPrec5 90.064 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:45<00:00,  2.17it/s]\n",
      "Train Epoch:2 | Loss 1.4020 | AdvPrec1 48.324 | AdvPrec5 92.950 | Reg term: 0.0 ||: 100%|██████████| 98/98 [00:46<00:00,  2.10it/s]\n",
      "Train Epoch:3 | Loss 1.2554 | AdvPrec1 54.114 | AdvPrec5 94.506 | Reg term: 0.0 ||:  32%|███▏      | 31/98 [00:15<00:34,  1.95it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7c63c0be8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/robustness/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(args, model, loaders, checkpoint, store, update_params, disable_no_grad)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         train_prec1, train_loss = _model_loop(args, 'train', train_loader, \n\u001b[0;32m--> 310\u001b[0;31m                 model, opt, epoch, args.adv_train, writer)\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mlast_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/robustness/train.py\u001b[0m in \u001b[0;36m_model_loop\u001b[0;34m(args, loop_type, loader, model, opt, epoch, adv, writer)\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0;34m'{1}1 {top1_acc:.3f} | {1}5 {top5_acc:.3f} | '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 'Reg term: {reg} ||'.format( epoch, prec, loop_msg, \n\u001b[0;32m--> 501\u001b[0;31m                 loss=losses, top1_acc=top1_acc, top5_acc=top5_acc, reg=reg_term))\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# USER-DEFINED HOOK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crit = ch.nn.CrossEntropyLoss()\n",
    "def custom_train_loss(logits, targ):\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    return train_crit(logits_to_multiply * logits, targ)\n",
    "\n",
    "adv_crit = ch.nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "def custom_adv_loss(model, inp, targ):\n",
    "    logits = model(inp)\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    new_logits = logits_to_multiply * logits\n",
    "    return adv_crit(new_logits, targ), new_logits\n",
    "\n",
    "train_args.custom_train_loss = custom_train_loss\n",
    "train_args.custom_adv_loss = custom_adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LambdaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness.loaders import LambdaLoader\n",
    "\n",
    "def label_noiser(ims, labels):\n",
    "    label_noise = ch.randint_like(labels, high=9)\n",
    "    probs = ch.ones_like(label_noise) * 0.1\n",
    "    labels_to_noise = ch.bernoulli(probs.float()).long()\n",
    "    new_labels = (labels + label_noise * labels_to_noise) % 10\n",
    "    return ims, new_labels\n",
    "\n",
    "train_loader = LambdaLoader(train_loader, label_noiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TransformedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness.loaders import TransformedLoader\n",
    "from robustness.data_augmentation import TRAIN_TRANSFORMS_DEFAULT\n",
    "\n",
    "def make_rand_labels(ims, targs):\n",
    "    new_targs = ch.randint(0, high=10,size=targs.shape).long()\n",
    "    return ims, new_targs\n",
    "\n",
    "train_loader_transformed = TransformedLoader(train_loader,\n",
    "                                            make_rand_labels,\n",
    "                                            TRAIN_TRANSFORMS_DEFAULT(32),\n",
    "                                            workers=8,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            do_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom per-iteration logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SCHEMA = {'iteration': int, 'weight_norm': float }\n",
    "out_store.add_table('custom', CUSTOM_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector as flatten\n",
    "\n",
    "def log_norm(mod, it, loop_type, inp, targ):\n",
    "    if loop_type == 'train':\n",
    "        curr_params = flatten(mod.parameters())\n",
    "        log_info_custom = { 'iteration': it,\n",
    "                    'weight_norm': ch.norm(curr_params).detach().cpu().numpy() }\n",
    "        out_store['custom'].append_row(log_info_custom)\n",
    "    \n",
    "train_args.iteration_hook = log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Must implement the num_classes argument\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        out = x.view(x.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "new_model = MLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, _ = make_and_restore_model(arch=new_model, dataset=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, new_model, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
